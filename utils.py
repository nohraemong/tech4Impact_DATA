import os
import logging
from pathlib import Path
from huggingface_hub import snapshot_download
import torch

logger = logging.getLogger(__name__)

class ModelDownloader:
    def __init__(self, cache_dir: str = "/tmp/model_cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)

    def download_model(
        self,
        model_id: str,
        force_download: bool = False
    ) -> str:
        """ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œì»¬ ê²½ë¡œ ë°˜í™˜"""
        
        model_cache_dir = self.cache_dir / model_id.replace("/", "--")
        
        # ì´ë¯¸ ë‹¤ìš´ë¡œë“œëœ ê²½ìš° í™•ì¸
        if model_cache_dir.exists() and not force_download:
            config_file = model_cache_dir / "config.json"
            if config_file.exists():
                logger.info(f"âœ… ìºì‹œëœ ëª¨ë¸ ì‚¬ìš©: {model_cache_dir}")
                return str(model_cache_dir)
        
        logger.info(f"ğŸ“¥ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹œì‘: {model_id}")
        logger.info(f"ğŸ’¾ ì €ì¥ ìœ„ì¹˜: {model_cache_dir}")
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë””ìŠ¤í¬ ê³µê°„ í™•ì¸
        free_space = self._get_free_space()
        logger.info(f"ğŸ’¿ ì‚¬ìš© ê°€ëŠ¥í•œ ê³µê°„: {free_space:.1f}GB")
        
        if free_space < 70:  # 32B ëª¨ë¸ìš© ìµœì†Œ ê³µê°„
            logger.warning(f"âš ï¸ ë””ìŠ¤í¬ ê³µê°„ ë¶€ì¡±: {free_space:.1f}GB < 70GB")
        
        try:
            # ë‹¤ìš´ë¡œë“œ ì‹¤í–‰
            downloaded_path = snapshot_download(
                repo_id=model_id,
                local_dir=str(model_cache_dir),
                local_dir_use_symlinks=False,
                resume_download=True,
                # ë¶ˆí•„ìš”í•œ íŒŒì¼ ì œì™¸
                ignore_patterns=[
                    "*.bin",  # safetensorsë§Œ ì‚¬ìš©
                    "pytorch_model*.bin",
                    "optimizer.pt",
                    "scheduler.pt",
                    "training_args.bin",
                    "*.msgpack",
                    "*.h5"
                ]
            )
            
            # ë‹¤ìš´ë¡œë“œ ì™„ë£Œ í™•ì¸
            model_size = self._calculate_directory_size(model_cache_dir)
            logger.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {model_size:.1f}GB")
            
            return str(model_cache_dir)
            
        except Exception as e:
            logger.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            # ì‹¤íŒ¨ ì‹œ ë¶€ë¶„ ë‹¤ìš´ë¡œë“œ ì •ë¦¬
            if model_cache_dir.exists():
                import shutil
                shutil.rmtree(model_cache_dir)
            raise

    def _get_free_space(self) -> float:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë””ìŠ¤í¬ ê³µê°„ (GB)"""
        statvfs = os.statvfs(self.cache_dir)
        return (statvfs.f_frsize * statvfs.f_bavail) / (1024**3)

    def _calculate_directory_size(self, directory: Path) -> float:
        """ë””ë ‰í† ë¦¬ í¬ê¸° ê³„ì‚° (GB)"""
        total_size = 0
        for file_path in directory.rglob('*'):
            if file_path.is_file():
                total_size += file_path.stat().st_size
        return total_size / (1024**3)

    def cleanup_old_models(self, keep_latest: int = 2):
        """ì˜¤ë˜ëœ ëª¨ë¸ ìºì‹œ ì •ë¦¬"""
        if not self.cache_dir.exists():
            return
            
        model_dirs = [d for d in self.cache_dir.iterdir() if d.is_dir()]
        if len(model_dirs) <= keep_latest:
            return
            
        # ìˆ˜ì • ì‹œê°„ ê¸°ì¤€ ì •ë ¬
        model_dirs.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        # ì˜¤ë˜ëœ ëª¨ë¸ ì œê±°
        for old_dir in model_dirs[keep_latest:]:
            logger.info(f"ğŸ—‘ï¸ ì˜¤ë˜ëœ ëª¨ë¸ ìºì‹œ ì œê±°: {old_dir}")
            import shutil
            shutil.rmtree(old_dir)

